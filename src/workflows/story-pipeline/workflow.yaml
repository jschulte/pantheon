name: story-pipeline
description: "Enhanced multi-agent pipeline with smart builder spawning, playbook learning, code citation evidence, test quality validation, resume-builder fixes, and per-story completion reports"
author: "Jonah Schulte (leveraging BMAD Method)"
version: "7.4.0" # Playbook Intelligence: compaction, index, budget loading, hit-rate tracking

# ============================================================================
# CRITICAL: EXECUTION DISCIPLINE
# ============================================================================
# This workflow MUST be invoked via the Skill tool (/bmad_pantheon_story-pipeline)
# The orchestrator (main Claude context) follows workflow.md phases.
#
# Task subagent_types listed below are ONLY for phases defined in workflow.md:
#   - Phase 2 BUILD: builder (Metis)
#   - Phase 3 VERIFY: inspector, test_quality, reviewers (Argus, Nemesis, etc.)
#   - Phase 4 ASSESS: arbiter (Themis)
#   - Phase 5 REFINE: builder resumed, reviewers resumed
#   - Phase 7 REFLECT: reflection (Mnemosyne)
#   - Phase 7 REPORT: story_reporter (Hermes) - generates completion summary
#
# DO NOT use Task tool outside the workflow phases!
# DO NOT spawn ad-hoc Task agents to "implement" stories directly!
# The workflow structure exists to ensure proper verification.
# ============================================================================

# Execution mode
execution_mode: "multi_agent" # multi_agent | single_agent (fallback)

# Critical variables from config
config_source: "{project-root}/_bmad/bmm/config.yaml"
output_folder: "{config_source}:output_folder"
sprint_artifacts: "{config_source}:sprint_artifacts"
communication_language: "{config_source}:communication_language"
date: system-generated

# Workflow paths
installed_path: "{project-root}/_pantheon/workflows/story-pipeline"
instructions: "{installed_path}/workflow.md"
agents_path: "{installed_path}/agents"

# =============================================================================
# SMART BUILDER SPAWNING (v5.0)
# =============================================================================
# Agent routing configuration determines which specialized builder to spawn
# based on story content, file patterns, and project configuration.
agent_routing:
  config_file: "{project-root}/_pantheon/agent-routing.yaml"
  builders_path: "{project-root}/_pantheon/agents/builders"
  reviewers_path: "{project-root}/_pantheon/agents/reviewers"
  validators_path: "{project-root}/_pantheon/agents/validators"

  # Project-level override (set in project's _bmad/_config/pantheon.yaml)
  # If set, always use this builder regardless of detection
  project_default_builder: null  # e.g., "frontend-react"

  # Available specialized builders (matched via agent-routing.yaml rules)
  available_builders:
    - id: frontend-react
      agent: "agents/builders/frontend-react.md"
      persona: { name: "Helios", emoji: "âš›ï¸" }
    - id: frontend-vue
      agent: "agents/builders/frontend-vue.md"
      persona: { name: "Verdant", emoji: "ğŸŒ¿" }
    - id: backend-typescript
      agent: "agents/builders/backend-typescript.md"
      persona: { name: "Hephaestus", emoji: "ğŸ”¥" }
    - id: backend-python
      agent: "agents/builders/backend-python.md"
      persona: { name: "Pythia", emoji: "ğŸ" }
    - id: backend-go
      agent: "agents/builders/backend-go.md"
      persona: { name: "Gopher", emoji: "ğŸ¦«" }
    - id: database-prisma
      agent: "agents/builders/database-prisma.md"
      persona: { name: "Athena", emoji: "ğŸ¦‰" }
    - id: database-sql
      agent: "agents/builders/database-sql.md"
      persona: { name: "Oracle", emoji: "ğŸ“Š" }
    - id: api-graphql
      agent: "agents/builders/api-graphql.md"
      persona: { name: "Mercury", emoji: "âš¡" }
    - id: infrastructure
      agent: "agents/builders/infrastructure.md"
      persona: { name: "Atlas", emoji: "ğŸŒ" }
    - id: testing
      agent: "agents/builders/testing.md"
      persona: { name: "Aletheia", emoji: "ğŸ¯" }
    - id: general
      agent: "agents/builders/general.md"
      persona: { name: "Metis", emoji: "ğŸ”¨" }

# =============================================================================
# PERSONA FORGING â€” Pygmalion (v7.0)
# =============================================================================
# Pygmalion analyzes code, stories, and playbooks to forge domain-specific
# specialist personas on-the-fly. Forged specialists augment (never replace)
# the fixed Pantheon reviewers.
persona_forging:
  enabled: true
  complexity_gate:
    skip_below: "light"          # trivial/micro use fixed Pantheon only
  max_specialists:
    light: 1
    standard: 2
    complex: 3
    critical: 4
  allow_forged_builder: false    # Only for complex+ when enabled
  model: "{model_tier.pygmalion}"  # sonnet â€” structured persona output
  timeout: 600                   # 10 minutes max
  output_dir: "{sprint_artifacts}/completions"
  registry:
    enabled: true
    directory: "docs/specialist-registry"
    index_file: "_index.json"
    match_thresholds:
      reuse: 0.5                 # Jaccard >= 0.5 â†’ reuse as-is
      evolve: 0.3                # 0.3 <= Jaccard < 0.5 â†’ evolve existing
    max_registry_size: 50        # Safety cap on total specialists

# =============================================================================
# REVIEWER LIMITS (v7.3)
# =============================================================================
# Maximum parallel reviewers for complex/critical stories.
# The orchestrator asks the user at runtime, using this as the default.
# 1 = consolidated (single multi-reviewer), 2 = two reviewers, 3 = three reviewers (recommended)
max_parallel_reviewers:
  default: 3            # Pre-selected option when asking user
  ask_at_runtime: true  # Prompt user before Phase 3 parallel review

# =============================================================================
# MODEL TIERS (v7.3)
# =============================================================================
# Controls which model each agent role uses.
# Opus for deep reasoning (code generation, security/logic analysis).
# Sonnet for structured/mechanical work (checklists, pattern matching, synthesis).
model_tier:
  builder: "opus"               # Writing correct code â€” mistakes are expensive
  fixer: "opus"                 # Resumed builder context â€” same reasoning needs
  cerberus: "opus"              # Security + logic â€” subtle bugs need deep reasoning
  themis: "opus"                # Triage arbiter â€” judgment calls on issue severity
  argus: "sonnet"               # Task verification â€” checklist/mechanical work
  nemesis: "sonnet"             # Test + quality review â€” pattern matching
  pygmalion: "sonnet"           # Persona forging â€” structured JSON output
  multi_reviewer: "opus"         # 4 perspectives in one pass â€” needs deep reasoning
  forged_specialists: "sonnet"  # Domain reviewers â€” structured checklist work
  reflection_reporter: "sonnet" # Synthesis â€” already sonnet

# Agent tracking (from GSD)
agent_history: "{sprint_artifacts}/agent-history.json"
current_agent_id: "{sprint_artifacts}/current-agent-id.txt"

# State management
state_file: "{sprint_artifacts}/story-pipeline-state-{{story_id}}.yaml"
audit_trail: "{sprint_artifacts}/audit-story-pipeline-{{story_id}}-{{date}}.yaml"

# Multi-agent configuration
agents:
  # ==========================================================================
  # HYBRID AGENT MAPPING (v5.1)
  # Strategy: Use Claude Code's specialized agents + layer Pantheon persona on top
  # See agent-type-mapping.md for full documentation
  # ==========================================================================

  pygmalion:
    description: "Persona Forge - analyzes domain, forges specialist personas on-the-fly"
    steps: [1.5]
    subagent_type: "general-purpose"
    model: "{model_tier.pygmalion}"  # sonnet â€” structured JSON output
    prompt_file: "{agents_path}/pygmalion.md"
    fresh_context: true
    trust_level: "high"
    timeout: 600
    output_file: "{sprint_artifacts}/completions/{{story_key}}-pygmalion.json"
    complexity_gate: "light"  # Skip for trivial/micro

  builder:
    description: "Implementation agent - writes code and tests (SMART SPAWNING)"
    steps: [1, 2, 3, 4]
    model: "{model_tier.builder}"  # opus â€” writing code needs deep reasoning
    # HYBRID: subagent_type is DYNAMICALLY SELECTED based on story content
    # React/Next.js â†’ dev-frontend + frontend-react.md
    # TypeScript API â†’ dev-typescript + backend-typescript.md
    # Python â†’ dev-python + backend-python.md
    # Go â†’ dev-go + backend-go.md
    # Database â†’ database-administrator + database-prisma.md
    # Infrastructure â†’ engineer-deployment + infrastructure.md
    # General â†’ general-purpose + general.md
    subagent_type_routing:
      frontend-react: "dev-frontend"
      frontend-vue: "dev-frontend"
      backend-typescript: "dev-typescript"
      backend-python: "dev-python"
      backend-go: "dev-go"
      database-prisma: "database-administrator"
      database-sql: "database-administrator"
      infrastructure: "engineer-deployment"
      general: "general-purpose"
    fallback_subagent: "general-purpose"
    smart_spawn: true
    routing_config: "{agent_routing.config_file}"
    fallback_agent: "agents/builders/general.md"
    prompt_file: "{agents_path}/builder.md"
    trust_level: "low"
    timeout: 3600

  inspector:
    description: "Validation agent - independent verification with code citations"
    steps: [5, 6]
    subagent_type: "general-purpose"  # No specialized inspector in Claude Code
    model: "{model_tier.argus}"  # sonnet â€” checklist/mechanical verification
    bmad_agent: "{validators_path}/inspector.md"  # Argus - The All-Seeing
    prompt_file: "{agents_path}/inspector.md"
    fresh_context: true
    trust_level: "medium"
    timeout: 1800
    require_code_citations: true

  test_quality:
    description: "Test quality validation - verifies test coverage and quality"
    steps: [5.5]
    subagent_type: "testing-suite:test-engineer"  # HYBRID: Claude Code test expertise
    model: "{model_tier.nemesis}"  # sonnet â€” pattern matching for test gaps
    bmad_agent: "{validators_path}/test-quality.md"  # Nemesis - Test quality persona
    prompt_file: "{agents_path}/test-quality.md"
    fresh_context: true
    trust_level: "medium"
    timeout: 1200

  reviewer:
    description: "Adversarial code review - finds problems (SMART ROUTING)"
    steps: [7]
    model: "{model_tier.cerberus}"  # opus for security/logic; see model_tier for overrides
    # HYBRID: Each reviewer type uses best Claude Code agent + Pantheon persona
    subagent_type_by_role:
      security: "auditor-security"        # OWASP expertise + Cerberus persona
      architecture: "architect-reviewer"  # SOLID/patterns + Hestia persona
      performance: "optimizer-performance" # Perf expertise + Apollo persona
      accessibility: "accessibility-expert" # WCAG expertise + Iris persona
      quality: "general-purpose"          # Flexibility + Arete persona
    smart_spawn: true
    routing_config: "{agent_routing.config_file}"
    fresh_context: true
    adversarial: true
    trust_level: "high"
    timeout: 1800

    # v7.3: reviewer count now controlled by max_parallel_reviewers (user-configurable)
    # These legacy counts are ignored when max_parallel_reviewers is set
    review_agent_count_legacy:
      micro: 2
      standard: 3
      complex: 4

    review_types:
      security: "{reviewers_path}/security.md"
      architecture: "{reviewers_path}/architecture.md"
      performance: "{reviewers_path}/performance.md"
      accessibility: "{reviewers_path}/accessibility.md"
      quality: "{reviewers_path}/quality.md"

    # Conditional reviewers (based on file patterns)
    # These are ADDED to the base count when triggered
    conditional_reviewers:
      ux_accessibility:
        name: "Ada"
        title: "The Accessibility Advocate"
        prompt_file: "{agents_path}/ux-accessibility-reviewer.md"
        trigger_on_files:
          - "*.tsx"
          - "*.jsx"
          - "*.vue"
          - "*.svelte"
          - "*.css"
          - "*.scss"
          - "*.sass"
          - "*.less"
          - "*.html"
          - "components/**"
          - "pages/**"
          - "app/**/page.tsx"
          - "app/**/layout.tsx"
          - "src/components/**"
          - "src/pages/**"
          - "src/views/**"
          - "public/**/*.html"
        trigger_on_keywords:
          - "component"
          - "UI"
          - "form"
          - "button"
          - "modal"
          - "dialog"
          - "accessibility"
          - "a11y"
          - "ARIA"
          - "screen reader"
          - "keyboard"
          - "focus"
        detection_command: |
          git diff --name-only HEAD~1 | grep -E "\.(tsx|jsx|vue|svelte|css|scss|html)$|components/|pages/|views/"

  fixer:
    description: "Issue resolution - Metis resumes to fix critical/high issues"
    steps: [8, 9]
    subagent_type: "general-purpose"
    bmad_agent: "{project-root}/_pantheon/agents/builder.md" # Metis resumes - "Measure twice, cut once"
    resume_builder: true # IMPORTANT: Resume Builder agent instead of spawning fresh
    prompt_file: "{agents_path}/fixer.md"
    trust_level: "low" # Same as builder â€” orchestrator handles commits
    timeout: 2400 # 40 minutes

  reflection_reporter:
    description: "Combined reflection + reporting - extracts learnings, updates playbooks, generates completion report"
    steps: [10, 11]
    subagent_type: "general-purpose"
    model: "sonnet" # Faster model sufficient for synthesis
    prompt_file: "{agents_path}/reflection-reporter.md" # Hermes combined
    timeout: 900 # 15 minutes
    outputs:
      - "{sprint_artifacts}/completions/{{story_key}}-mnemosyne.json"
      - "{sprint_artifacts}/completions/{{story_key}}-summary.md"
      - "{sprint_artifacts}/completions/{{story_key}}-hermes.json"
    token_savings: "~5-8K tokens vs separate agents"

  multi_reviewer:
    description: "Consolidated multi-perspective review - Argus + Nemesis + Cerberus + Hestia in one pass"
    steps: [5, 6, 7]
    subagent_type: "general-purpose"
    model: "{model_tier.multi_reviewer}"  # opus â€” 4 perspectives needs deep reasoning
    prompt_file: "{agents_path}/multi-reviewer.md"
    timeout: 2400 # 40 minutes (doing work of 4 agents)
    output_file: "{sprint_artifacts}/completions/{{story_key}}-review.json"
    token_savings: "~60-70% reduction vs parallel reviewers"
    use_when:
      - "trivial"
      - "micro"
      - "light"
      - "standard"

# Reconciliation: orchestrator does this directly (see workflow.md Phase 6)

# Playbook configuration (v4.0, enhanced v7.4)
playbooks:
  enabled: true # Set to false in project config to disable
  directory: "docs/implementation-playbooks"
  index_file: "_index.json" # Structured playbook index for budget-based loading
  bootstrap_mode: true # Auto-initialize if missing
  token_budget: 7500 # Max tokens of playbook content to load (~30KB)
  target_size_bytes: [3000, 10000] # Per-playbook size target (3-10KB)
  compaction_threshold: 10000 # Bytes â€” above this triggers mandatory compaction
  auto_apply_updates: false # Require manual review of playbook updates
  discovery:
    enabled: true # Scan git/docs to populate initial playbooks
    sources: ["git_history", "docs", "existing_code"]

# Quality gates (v4.0)
quality_gates:
  coverage_threshold: 80 # % line coverage required
  task_verification: "all_with_evidence" # Inspector must provide file:line citations
  critical_issues: "must_fix"
  high_issues: "must_fix"

# Complexity level (determines which steps to execute)
complexity_level: "standard" # micro | standard | complex

# Complexity routing (v4.2 - token-optimized)
complexity_routing:
  trivial:
    review_mode: "consolidated" # Use multi_reviewer
    description: "Minimal review for static/config changes"
    examples: ["README update", "config change", "copy fix"]

  micro:
    review_mode: "consolidated" # Use multi_reviewer
    description: "Lightweight path for low-risk stories"
    examples: ["UI tweaks", "text changes", "simple CRUD"]

  light:
    review_mode: "consolidated" # Use multi_reviewer
    description: "Standard path with consolidated review"
    examples: ["Simple component", "basic form"]

  standard:
    review_mode: "consolidated" # Use multi_reviewer (saves ~25K tokens)
    description: "Balanced path - consolidated review sufficient"
    examples: ["API endpoints", "business logic"]

  complex:
    review_mode: "parallel" # Reviewer count set by max_parallel_reviewers (user choice)
    description: "Enhanced validation - consolidated parallel reviewers"
    examples: ["Auth", "payments", "security", "migrations"]

  critical:
    review_mode: "parallel" # Reviewer count set by max_parallel_reviewers (user choice)
    description: "Maximum scrutiny - consolidated parallel reviewers"
    examples: ["Payment processing", "encryption", "PII handling"]

# Token efficiency summary (v7.3):
# - trivial/micro/light/standard: Use multi_reviewer (saves ~60-70% of Phase 3 tokens)
# - complex/critical: Use max_parallel_reviewers consolidated reviewers (user-configurable)
#   - 3 (default): Argus+Hestia, Cerberus+Apollo, Nemesis+Arete
#   - 2: Argus+Hestia+Arete, Cerberus+Apollo+Nemesis
#   - 1: Falls through to consolidated multi-reviewer
# - All tiers: Digest-first context (inline only top-5 files, Read tool for rest)
# - All tiers: Use reflection_reporter instead of separate Mnemosyne + Hermes (saves ~5-8K)

# Final verification checklist (main orchestrator)
final_verification:
  enabled: true
  checks:
    - name: "git_commits"
      command: "git log --oneline -3 | grep {{story_key}}"
      failure_message: "No commit found for {{story_key}}"

    - name: "story_checkboxes"
      command: |
        before=$(git show HEAD~1:{{story_file}} | grep -c '^- \[x\]')
        after=$(grep -c '^- \[x\]' {{story_file}})
        [ $after -gt $before ]
      failure_message: "Story checkboxes not updated"

    - name: "sprint_status"
      command: "git diff HEAD~1 {{sprint_status}} | grep '{{story_key}}'"
      failure_message: "Sprint status not updated"

    - name: "tests_passed"
      # Parse agent output for test evidence
      validation: "inspector_output must contain 'PASS' or test count"
      failure_message: "No test evidence in validation output"

# Backward compatibility
fallback_to_v1:
  enabled: true
  condition: "execution_mode == 'single_agent'"
  workflow: "{project-root}/_pantheon/workflows/story-pipeline"

standalone: true
