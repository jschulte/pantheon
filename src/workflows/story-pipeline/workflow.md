# Story-Full-Pipeline v4.0 - Enhanced Multi-Agent Pipeline

<purpose>
Implement a story using parallel verification agents with Builder context reuse.
Enhanced with playbook learning, code citation evidence, test quality validation, and automated coverage gates.
Builder fixes issues in its own context (50-70% token savings).
</purpose>

<philosophy>
**Quality Through Discipline, Continuous Learning**

- Story Quality Gate: Fresh validation before committing resources
- Playbook Query: Load relevant patterns before starting
- Mason: Implements with playbook knowledge (context preserved)
- Vera + Tessa + Rex: Validate in parallel with evidence
- Coverage Gate: Automated threshold enforcement
- Mason: Refines issues in same context (50-70% token savings)
- Vera: Quick recheck
- Orchestrator: Reconciles mechanically
- Rita: Updates playbooks for future agents

Measure twice, cut once. Trust but verify. Evidence-based validation. Self-improving system.
</philosophy>

<config>
name: story-pipeline
version: 4.2.0
execution_mode: multi_agent

phases:
  phase_0: Story Quality Gate (orchestrator validates story readiness)
  phase_0.5: Playbook Query (orchestrator)
  phase_1: Mason the Craftsman (saves agent_id)
  phase_2: [Vera + Tessa + Rex reviewers] in parallel
  phase_2.5: Coverage Gate (automated)
  phase_3: Resume Mason with all findings (reuses context)
  phase_4: Vera re-check (quick verification)
  phase_5: Orchestrator reconciliation
  phase_6: Rita reflection (playbook updates)

reviewer_counts:
  micro: 2 reviewers (security, architect/integration)
  standard: 3 reviewers (security, logic/performance, architect/integration)
  complex: 4 reviewers (security, logic, architect/integration, code quality)

quality_gates:
  coverage_threshold: 80  # % line coverage required
  task_verification: "all_with_evidence"  # Inspector must cite file:line
  critical_issues: "must_fix"
  high_issues: "must_fix"

token_efficiency:
  - Phase 2 agents spawn in parallel (same cost, faster)
  - Phase 3 resumes Builder (50-70% token savings vs fresh agent)
  - Phase 4 Inspector only (no full re-review)

playbooks:
  enabled: true
  directory: "docs/playbooks/implementation-playbooks"
  max_load: 3
  auto_apply_updates: false
</config>

<execution_context>
@patterns/verification.md
@patterns/tdd.md
@patterns/agent-completion.md
</execution_context>

<process>

<step name="load_story" priority="first">
**Load and parse story file**

\`\`\`bash
STORY_FILE="docs/sprint-artifacts/{{story_key}}.md"
[ -f "$STORY_FILE" ] || { echo "ERROR: Story file not found"; exit 1; }
\`\`\`

Use Read tool. Extract:
- Task count
- Acceptance criteria count
- Keywords for risk scoring

**Determine complexity:**
\`\`\`bash
TASK_COUNT=$(grep -c "^- \[ \]" "$STORY_FILE")
RISK_KEYWORDS=$(grep -ciE "auth|security|payment|encryption|migration|database" "$STORY_FILE")

if [ "$TASK_COUNT" -le 3 ] && [ "$RISK_KEYWORDS" -eq 0 ]; then
  COMPLEXITY="micro"
  REVIEWER_COUNT=2
elif [ "$TASK_COUNT" -ge 16 ] || [ "$RISK_KEYWORDS" -gt 0 ]; then
  COMPLEXITY="complex"
  REVIEWER_COUNT=4
else
  COMPLEXITY="standard"
  REVIEWER_COUNT=3
fi
\`\`\`

Determine agents to spawn: Inspector + Test Quality + $REVIEWER_COUNT Reviewers
</step>

<step name="story_quality_gate">
**Phase 0: Story Quality Gate**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” PHASE 0: STORY QUALITY GATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Fresh validation before committing dev resources
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

**Orchestrator performs these checks directly (no Task spawn):**

Read the story file with fresh eyes and validate:

**1. Required Sections Exist:**
\`\`\`bash
# Check for essential story structure
grep -q "## Story Title\|## Story:" "$STORY_FILE" || echo "âŒ MISSING: Story Title"
grep -q "## Business Context\|## Context\|## Background" "$STORY_FILE" || echo "âš ï¸  MISSING: Business Context"
grep -q "## Acceptance Criteria\|## AC\|## Definition of Done" "$STORY_FILE" || echo "âŒ MISSING: Acceptance Criteria"
grep -q "## Tasks\|## Implementation Tasks\|^- \[ \]" "$STORY_FILE" || echo "âŒ MISSING: Tasks"
\`\`\`

**2. Tasks Are Well-Defined (not placeholders):**
\`\`\`bash
# Check for placeholder tasks
PLACEHOLDER_TASKS=$(grep -E "^\- \[ \] (TBD|TODO|WIP|Placeholder|...)" "$STORY_FILE" | wc -l)
if [ "$PLACEHOLDER_TASKS" -gt 0 ]; then
  echo "âŒ BLOCKER: $PLACEHOLDER_TASKS placeholder tasks found"
  grep -E "^\- \[ \] (TBD|TODO|WIP|Placeholder|...)" "$STORY_FILE"
fi

# Check tasks have meaningful descriptions (> 10 chars)
SHORT_TASKS=$(grep "^- \[ \]" "$STORY_FILE" | awk 'length($0) < 20' | wc -l)
if [ "$SHORT_TASKS" -gt 0 ]; then
  echo "âš ï¸  WARNING: $SHORT_TASKS tasks may be too brief"
fi
\`\`\`

**3. No Unresolved Blockers:**
\`\`\`bash
# Check for blocker markers
BLOCKERS=$(grep -ciE "\[BLOCKER\]|\[BLOCKED\]|\[NEEDS.DECISION\]|\[NEEDS.CLARIFICATION\]|\[UNRESOLVED\]" "$STORY_FILE")
if [ "$BLOCKERS" -gt 0 ]; then
  echo "âŒ BLOCKER: $BLOCKERS unresolved blockers found"
  grep -iE "\[BLOCKER\]|\[BLOCKED\]|\[NEEDS.DECISION\]|\[NEEDS.CLARIFICATION\]|\[UNRESOLVED\]" "$STORY_FILE"
fi
\`\`\`

**4. Acceptance Criteria Are Testable:**
\`\`\`bash
# Count acceptance criteria
AC_COUNT=$(grep -cE "^\- \[ \].*should|^\- \[ \].*must|^\- \[ \].*can|^- Given.*When.*Then" "$STORY_FILE" || echo "0")
if [ "$AC_COUNT" -eq 0 ]; then
  echo "âš ï¸  WARNING: No testable acceptance criteria found (should/must/can/Given-When-Then)"
fi
\`\`\`

**5. Gap Analysis Status (brownfield check):**
\`\`\`bash
# If story references existing code, check gap analysis was done
if grep -q "existing\|modify\|update\|extend\|refactor" "$STORY_FILE"; then
  if ! grep -q "Gap Analysis\|Codebase Analysis\|## Existing Code" "$STORY_FILE"; then
    echo "âš ï¸  WARNING: Story references existing code but no gap analysis section found"
  fi
fi
\`\`\`

**Quality Gate Decision:**
\`\`\`
IF any âŒ BLOCKER found:
  â†’ HALT pipeline
  â†’ Report issues to user
  â†’ Suggest: "Run /bmad_bmm_validate to fix story issues"

IF only âš ï¸ WARNINGs found:
  â†’ Display warnings to user
  â†’ ASK: "Proceed despite warnings? [y/N]"
  â†’ If user declines, halt pipeline

IF all checks pass:
  â†’ Display "âœ… Story quality gate passed"
  â†’ Proceed to Phase 0.5 (Playbook Query)
\`\`\`
</step>

<step name="query_playbooks">
**Phase 0.5: Playbook Query**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š PHASE 0.5: PLAYBOOK QUERY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

**Extract story keywords:**
\`\`\`bash
STORY_KEYWORDS=$(grep -E "^## Story Title|^### Feature|^## Business Context" "$STORY_FILE" | sed 's/[#]//g' | tr '\n' ' ')
echo "Story keywords: $STORY_KEYWORDS"
\`\`\`

**Search for relevant playbooks:**
Use Grep tool:
- Pattern: extracted keywords
- Path: \`docs/playbooks/implementation-playbooks/\`
- Output mode: files_with_matches
- Limit: 3 files

**Load matching playbooks:**
For each playbook found:
- Use Read tool
- Extract sections: Common Gotchas, Code Patterns, Test Requirements

If no playbooks exist:
\`\`\`
â„¹ï¸  No playbooks found - this will be the first story to create them
\`\`\`

Store playbook content for Builder.
</step>

<step name="spawn_builder">
**Phase 1: Builder Agent (Mason ğŸ”¨)**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¨ PHASE 1: MASON THE CRAFTSMAN
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

**1. Load BMAD Agent Persona:**
Read: \`{project-root}/_bmad/bse/agents/builder.md\`
Extract the \`<persona>\` section - this defines WHO Mason is.

**2. Spawn Builder agent and SAVE agent_id for resume later:**

âš ï¸ **This Task should be VISIBLE in Claude Code UI!**

\`\`\`
BUILDER_TASK = Task({
  subagent_type: "general-purpose",
  model: "opus",
  description: "ğŸ”¨ Mason the Craftsman on {{story_key}}",
  prompt: \`
You are MASON ğŸ”¨ - The Craftsman.

<persona>
[INJECT persona section from _bmad/bse/agents/builder.md]
</persona>

You are implementing story {{story_key}}.

<execution_context>
@patterns/tdd.md
@patterns/agent-completion.md
</execution_context>

<context>
Story: [inline story file content]

{{IF playbooks loaded}}
Relevant Playbooks (review before implementing):
[inline playbook content]

Pay special attention to:
- Common Gotchas in these playbooks
- Code Patterns to follow
- Test Requirements to satisfy
{{ENDIF}}
</context>

<objective>
Implement the story requirements:
1. Review story tasks and acceptance criteria
2. **Review playbooks** for gotchas and patterns (if provided)
3. Analyze what exists vs needed (gap analysis)
4. **Write tests FIRST** (TDD - tests before implementation)
5. Implement production code to pass tests
</objective>

<constraints>
- Run tests and linting before finishing (take pride in your work!)
- Self-review, but don't self-certify as "complete" (Inspector verifies)
- DO NOT update story checkboxes (Orchestrator does this)
- DO NOT commit changes yet (happens after review)
</constraints>

<success_criteria>
- [ ] Reviewed playbooks for guidance
- [ ] Tests written for all requirements
- [ ] Production code implements tests
- [ ] Tests pass
- [ ] Return structured completion artifact
</success_criteria>

<completion_format>
Return structured JSON artifact:
{
  "agent": "builder",
  "story_key": "{{story_key}}",
  "status": "SUCCESS" | "FAILED",
  "files_created": ["path/to/file.tsx", ...],
  "files_modified": ["path/to/file.tsx", ...],
  "tests_added": {
    "total": 12,
    "passing": 12
  },
  "tasks_addressed": ["task description from story", ...]
}

Save to: docs/sprint-artifacts/completions/{{story_key}}-builder.json
</completion_format>
\`
})

BUILDER_AGENT_ID = {{extract agent_id from Task result}}
\`\`\`

**CRITICAL: Store Builder agent ID:**
\`\`\`bash
echo "Builder agent ID: $BUILDER_AGENT_ID"
echo "$BUILDER_AGENT_ID" > /tmp/builder-agent-id.txt
\`\`\`

**Wait for completion. Verify artifact exists:**
\`\`\`bash
BUILDER_COMPLETION="docs/sprint-artifacts/completions/{{story_key}}-builder.json"
[ -f "$BUILDER_COMPLETION" ] || { echo "âŒ No builder artifact"; exit 1; }
\`\`\`

**Verify files exist:**
\`\`\`bash
# For each file in files_created and files_modified:
[ -f "$file" ] || echo "âŒ MISSING: $file"
\`\`\`

If files missing or status FAILED: halt pipeline.
</step>

<step name="spawn_verification_parallel">
**Phase 2: Parallel Verification (Vera ğŸ” + Tessa ğŸ§ª + Rex ğŸ”´)**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” PHASE 2: PARALLEL VERIFICATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Spawning: Vera (Inspector) + Tessa (Test Quality) + {{REVIEWER_COUNT}} Rex clones (Reviewers)
Total agents: {{2 + REVIEWER_COUNT}}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

**Load BMAD Agent Personas (before spawning):**
- Read: \`{project-root}/_bmad/bse/agents/inspector.md\` â†’ Extract \`<persona>\` for Vera
- Read: \`{project-root}/_bmad/bse/agents/test-quality.md\` â†’ Extract \`<persona>\` for Tessa
- Read: \`{project-root}/_bmad/bse/agents/reviewer.md\` â†’ Extract \`<persona>\` for Rex

**CRITICAL: Spawn ALL agents in ONE message (parallel execution)**

Send single message with multiple Task calls:
1. Vera ğŸ” (Inspector)
2. Tessa ğŸ§ª (Test Quality)
3. Rex ğŸ”´ - Security focus
4. Rex ğŸ”´ - Logic/Performance focus (if standard/complex)
5. Rex ğŸ”´ - Architecture focus
6. Rex ğŸ”´ - Code Quality focus (if complex)

---

## Vera ğŸ” (Inspector) Prompt:

\`\`\`
Task({
  subagent_type: "testing-suite:test-engineer",
  model: "opus",
  description: "ğŸ•µï¸ Vera the Inspector on {{story_key}}",
  prompt: \`
You are VERA ğŸ” - The Verification Inspector.

<persona>
[INJECT persona section from _bmad/bse/agents/inspector.md]
</persona>

You are verifying story {{story_key}}.

<execution_context>
@patterns/verification.md
@patterns/agent-completion.md
</execution_context>

<context>
Story: [inline story file content]
</context>

<objective>
Independently verify implementation WITH CODE CITATIONS:

1. Read story file - understand ALL tasks
2. Read each file Builder created/modified
3. **Map EACH task to specific code with file:line citations**
4. Run verification checks:
   - Type-check (0 errors required)
   - Lint (0 warnings required)
   - Tests (all passing required)
   - Build (success required)
</objective>

<critical_requirement>
**EVERY task must have evidence.**

For each task, provide:
- file:line where it's implemented
- Brief quote of relevant code
- Verdict: IMPLEMENTED or NOT_IMPLEMENTED

Example:
Task: "Display occupant agreement status"
Evidence: src/features/agreement/StatusBadge.tsx:45-67
Code: "const StatusBadge = ({ status }) => ..."
Verdict: IMPLEMENTED
</critical_requirement>

<constraints>
- You have NO KNOWLEDGE of what Builder did
- Run all checks yourself - don't trust claims
- **Every task needs file:line citation**
- If code doesn't exist: mark NOT IMPLEMENTED with reason
</constraints>

<success_criteria>
- [ ] ALL tasks mapped to code locations
- [ ] Type check: 0 errors
- [ ] Lint: 0 warnings
- [ ] Tests: all passing
- [ ] Build: success
- [ ] Return structured evidence
</success_criteria>

<completion_format>
{
  "agent": "inspector",
  "story_key": "{{story_key}}",
  "verdict": "PASS" | "FAIL",
  "task_verification": [
    {
      "task": "Create agreement view component",
      "implemented": true,
      "evidence": [
        {
          "file": "src/features/agreement/AgreementView.tsx",
          "lines": "15-67",
          "code_snippet": "export const AgreementView = ({ agreementId }) => {...}"
        },
        {
          "file": "src/features/agreement/AgreementView.test.tsx",
          "lines": "8-45",
          "code_snippet": "describe('AgreementView', () => {...})"
        }
      ]
    },
    {
      "task": "Add status badge",
      "implemented": false,
      "evidence": [],
      "reason": "No StatusBadge component found in src/features/agreement/"
    }
  ],
  "checks": {
    "type_check": {"passed": true, "errors": 0},
    "lint": {"passed": true, "warnings": 0},
    "tests": {"passed": true, "total": 12, "passing": 12},
    "build": {"passed": true}
  }
}

Save to: docs/sprint-artifacts/completions/{{story_key}}-inspector.json
</completion_format>
\`
})
\`\`\`

---

## Tessa ğŸ§ª (Test Quality) Prompt:

\`\`\`
Task({
  subagent_type: "testing-suite:test-engineer",
  model: "opus",
  description: "ğŸ§ª Tessa the Test Scientist on {{story_key}}",
  prompt: \`
You are TESSA ğŸ§ª - The Test Quality Analyst.

<persona>
[INJECT persona section from _bmad/bse/agents/test-quality.md]
</persona>

You are reviewing test quality for story {{story_key}}.

<context>
Story: [inline story file content]
Builder completion: [inline builder artifact]
</context>

<objective>
Review test files for quality and completeness:

1. Find all test files created/modified by Builder
2. For each test file, verify:
   - **Happy path**: Primary functionality tested âœ“
   - **Edge cases**: null, empty, invalid inputs âœ“
   - **Error conditions**: Failures handled properly âœ“
   - **Assertions**: Meaningful checks (not just "doesn't crash")
   - **Test names**: Descriptive and clear
   - **Deterministic**: No random data, no timing dependencies
3. Check that tests actually validate the feature

**Focus on:** What's missing? What edge cases weren't considered?
</objective>

<success_criteria>
- [ ] All test files reviewed
- [ ] Edge cases identified (covered or missing)
- [ ] Error conditions verified
- [ ] Assertions are meaningful
- [ ] Tests are deterministic
- [ ] Return quality assessment
</success_criteria>

<completion_format>
{
  "agent": "test_quality",
  "story_key": "{{story_key}}",
  "verdict": "PASS" | "NEEDS_IMPROVEMENT",
  "test_files_reviewed": ["path/to/test.tsx", ...],
  "issues": [
    {
      "severity": "HIGH",
      "file": "path/to/test.tsx:45",
      "issue": "Missing edge case: empty input array",
      "recommendation": "Add test: expect(fn([])).toThrow(...)"
    },
    {
      "severity": "MEDIUM",
      "file": "path/to/test.tsx:67",
      "issue": "Test uses Math.random() - could be flaky",
      "recommendation": "Use fixed test data"
    }
  ],
  "coverage_analysis": {
    "edge_cases_covered": true | false,
    "error_conditions_tested": true | false,
    "meaningful_assertions": true | false,
    "tests_are_deterministic": true | false
  },
  "summary": {
    "high_issues": 1,
    "medium_issues": 2,
    "low_issues": 0
  }
}

Save to: docs/sprint-artifacts/completions/{{story_key}}-test-quality.json
</completion_format>
\`
})
\`\`\`

---

## Rex ğŸ”´ Reviewers (Spawn based on complexity)

Load Rex's persona from: `{project-root}/_bmad/bse/agents/reviewer.md`

**Security Reviewer (always spawn):**
\`\`\`
Task({
  subagent_type: "auditor-security",
  model: "opus",
  description: "ğŸ”´ Sasha (Security) on {{story_key}}",
  prompt: \`
You are SASHA ğŸ”´ - The Security Sentinel.

<persona>
[INJECT persona section from _bmad/bse/agents/reviewer.md]
</persona>

Focus: Security vulnerabilities, injection attacks, auth issues.
[... security review prompt ...]
\`
})
\`\`\`

**Logic/Performance Reviewer (standard/complex):**
\`\`\`
Task({
  subagent_type: "optimizer-performance",
  model: "opus",
  description: "ğŸ”´ Leo (Logic) on {{story_key}}",
  prompt: "... logic and performance review ..."
})
\`\`\`

**Architecture Reviewer (always spawn):**
\`\`\`
Task({
  subagent_type: "architect-reviewer",
  model: "opus",
  description: "ğŸ”´ Rosie (Architecture) on {{story_key}}",
  prompt: "... architecture patterns review ..."
})
\`\`\`

**Code Quality Reviewer (complex only):**
\`\`\`
Task({
  subagent_type: "general-purpose",
  model: "opus",
  description: "ğŸ”´ Quinn (Quality) on {{story_key}}",
  prompt: "... code quality review ..."
})
\`\`\`

---

**Wait for ALL agents to complete.**

Collect completion artifacts:
- \`inspector.json\`
- \`test-quality.json\`
- \`reviewer-security.json\`
- \`reviewer-logic.json\` (if spawned)
- \`reviewer-architect.json\`
- \`reviewer-quality.json\` (if spawned)

Parse all findings and aggregate by severity.
</step>

<step name="coverage_gate">
**Phase 2.5: Coverage Gate (Automated)**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š PHASE 2.5: COVERAGE GATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

Run coverage check:
\`\`\`bash
# Run tests with coverage
npm test -- --coverage --silent 2>&1 | tee coverage-output.txt

# Extract coverage percentage (adjust grep pattern for your test framework)
COVERAGE=$(grep -E "All files|Statements" coverage-output.txt | head -1 | grep -oE "[0-9]+\.[0-9]+|[0-9]+" | head -1 || echo "0")

echo "Coverage: ${COVERAGE}%"
echo "Threshold: {{coverage_threshold}}%"

# Compare coverage
if (( $(echo "$COVERAGE < {{coverage_threshold}}" | bc -l) )); then
  echo "âŒ Coverage ${COVERAGE}% below threshold {{coverage_threshold}}%"
  echo "Builder must add more tests before proceeding"
  exit 1
fi

echo "âœ… Coverage gate passed: ${COVERAGE}%"
\`\`\`

If coverage fails: add to issues list for Builder to fix.
</step>

<step name="resume_builder_with_findings">
**Phase 3: Resume Mason ğŸ”¨ with All Findings**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¨ PHASE 3: MASON REFINES THE WORK
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

**CRITICAL: Resume Mason (reuses context, 50-70% token savings!)**

âš ï¸ **This Task should be VISIBLE in Claude Code UI!**

\`\`\`
Task({
  subagent_type: "general-purpose",
  model: "opus",
  description: "ğŸ”¨ Mason refining {{story_key}}",
  resume: "{{BUILDER_AGENT_ID}}",
  prompt: \`
Mason, the review team found some areas that need refinement.

<issues_to_fix>
[List all CRITICAL and HIGH issues with file:line citations]
</issues_to_fix>

<craftsman_principles>
"A true craftsman takes feedback as an opportunity to improve the work."
- Address issues in priority order (CRITICAL â†’ HIGH â†’ MEDIUM)
- Run tests after each refinement to confirm resolution
- Commit with descriptive message when done
</craftsman_principles>

<skip>
- LOW priority items (gold-plating, can address later)
</skip>
\`\`\`

Mason addresses the issues then returns:
\`\`\`json
{
  "agent": "builder_fixes",
  "fixes_applied": [
    {"issue": "SQL injection", "file": "api/route.ts:45", "fix": "Used parameterized query"},
    ...
  ],
  "tests_passing": true,
  "commit_hash": "abc123"
}
\`\`\`

Wait for completion. Parse commit hash and fix counts.
</step>

<step name="inspector_recheck">
**Phase 4: Vera ğŸ•µï¸ Quick Re-Check**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ•µï¸ PHASE 4: VERA RE-VERIFIES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

âš ï¸ **This Task should be VISIBLE in Claude Code UI!**

\`\`\`
Task({
  subagent_type: "testing-suite:test-engineer",
  model: "opus",
  description: "ğŸ•µï¸ Vera re-checking {{story_key}}",
  prompt: \`
You are VERA ğŸ•µï¸ - Quick re-verification after Mason's refinements.

Verify that:
1. All CRITICAL/HIGH issues from Phase 2 are resolved
2. Tests still pass
3. No new issues introduced

Return: PASS or FAIL with evidence.
\`
})
\`\`\`

If FAIL: Resume Builder again with new issues.
If PASS: Proceed to reconciliation.
</step>

<step name="reconcile_story">
**Phase 5: Orchestrator Reconciliation**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š PHASE 5: RECONCILIATION (Orchestrator)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

**YOU (orchestrator) do this directly. No agent spawn.**

**5.1: Load completion artifacts**
\`\`\`bash
BUILDER_FIXES="docs/sprint-artifacts/completions/{{story_key}}-builder-fixes.json"
INSPECTOR="docs/sprint-artifacts/completions/{{story_key}}-inspector.json"
\`\`\`

Use Read tool on all artifacts.

**5.2: Read story file**
Use Read tool: \`docs/sprint-artifacts/{{story_key}}.md\`

**5.3: Check off completed tasks using Inspector evidence**

For each task in \`inspector.task_verification\`:
- If \`implemented: true\` and has evidence:
  - Use Edit tool: \`"- [ ] {{task}}"\` â†’ \`"- [x] {{task}}"\`

**5.4: Fill Dev Agent Record with evidence**

Use Edit tool:
\`\`\`text
**Dev Agent Record**
**Implementation Date:** {{timestamp}}
**Agent Model:** Claude Sonnet 4.5 (multi-agent pipeline v4.0)
**Git Commit:** {{git_commit}}

**Pipeline Phases:**
- Phase 0: Playbook Query ({{playbooks_loaded}} loaded)
- Phase 1: Builder (initial implementation)
- Phase 2: Parallel Verification
  - Inspector: {{verdict}} with code citations
  - Test Quality: {{verdict}}
  - {{REVIEWER_COUNT}} Reviewers: {{issues_found}}
- Phase 2.5: Coverage Gate ({{coverage}}%)
- Phase 3: Builder (resumed, fixed {{fixes_count}} issues)
- Phase 4: Inspector re-check ({{verdict}})

**Files Created:** {{count}}
**Files Modified:** {{count}}
**Tests:** {{tests.passing}}/{{tests.total}} passing ({{coverage}}%)
**Issues Fixed:** {{critical}} CRITICAL, {{high}} HIGH, {{medium}} MEDIUM

**Task Evidence:** (Inspector code citations)
{{for each task with evidence}}
- [x] {{task}}
  - {{evidence[0].file}}:{{evidence[0].lines}}
{{endfor}}
\`\`\`

**5.5: Verify updates**
\`\`\`bash
CHECKED=$(grep -c "^- \[x\]" docs/sprint-artifacts/{{story_key}}.md)
[ "$CHECKED" -gt 0 ] || { echo "âŒ Zero tasks checked"; exit 1; }
echo "âœ… Reconciled: $CHECKED tasks with evidence"
\`\`\`
</step>

<step name="final_verification">
**Final Quality Gate**

\`\`\`bash
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ” FINAL VERIFICATION"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# 1. Git commit exists
git log --oneline -3 | grep "{{story_key}}" || { echo "âŒ No commit"; exit 1; }
echo "âœ… Git commit found"

# 2. Story tasks checked with evidence
CHECKED=$(grep -c "^- \[x\]" docs/sprint-artifacts/{{story_key}}.md)
[ "$CHECKED" -gt 0 ] || { echo "âŒ No tasks checked"; exit 1; }
echo "âœ… $CHECKED tasks checked with code citations"

# 3. Dev Agent Record filled
grep -A 5 "### Dev Agent Record" docs/sprint-artifacts/{{story_key}}.md | grep -q "202" || { echo "âŒ Record not filled"; exit 1; }
echo "âœ… Dev Agent Record filled"

# 4. Coverage met threshold
FINAL_COVERAGE=$(jq -r '.tests.coverage' docs/sprint-artifacts/completions/{{story_key}}-builder-fixes.json)
if (( $(echo "$FINAL_COVERAGE < {{coverage_threshold}}" | bc -l) )); then
  echo "âŒ Coverage ${FINAL_COVERAGE}% still below threshold"
  exit 1
fi
echo "âœ… Coverage: ${FINAL_COVERAGE}%"

echo ""
echo "âœ… STORY COMPLETE"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
\`\`\`

**Update sprint-status.yaml:**
Use Edit tool: \`"{{story_key}}: ready-for-dev"\` â†’ \`"{{story_key}}: done"\`

**Commit validated changes:**
\`\`\`bash
# Stage story file and sprint status updates
git add docs/sprint-artifacts/{{story_key}}.md
git add docs/sprint-artifacts/sprint-status.yaml
git add docs/sprint-artifacts/completions/

# Create reconciliation commit
git commit -m "$(cat <<'EOF'
chore({{story_key}}): reconcile story completion

- Check off completed tasks with code citations
- Fill Dev Agent Record with pipeline results
- Update sprint-status to done
- Archive completion artifacts
EOF
)"

echo "âœ… Reconciliation committed"
\`\`\`

**Verify commit:**
\`\`\`bash
git log --oneline -1 | grep -q "reconcile" || { echo "âŒ Reconciliation commit failed"; exit 1; }
echo "âœ… Reconciliation commit verified"
\`\`\`
</step>

<step name="playbook_reflection">
**Phase 6: Playbook Reflection**

\`\`\`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ PHASE 6: PLAYBOOK REFLECTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
\`\`\`

Spawn Reflection Agent:

\`\`\`
Task({
  subagent_type: "general-purpose",
  model: "opus",
  description: "ğŸ“š Rita the Librarian reflecting on {{story_key}}",
  prompt: \`
You are the REFLECTION agent for story {{story_key}}.

<context>
Story: [inline story file]
Builder initial: [inline builder.json]
All review findings: [inline all reviewer artifacts]
Builder fixes: [inline builder-fixes.json]
Test quality issues: [inline test-quality.json]
</context>

<objective>
Identify what future agents should know:

1. **What issues were found?** (from reviewers)
2. **What did Builder miss initially?** (gaps, edge cases, security)
3. **What playbook knowledge would have prevented these?**
4. **Which module/feature area does this apply to?**
5. **Should we update existing playbook or create new?**

Questions:
- What gotchas should future builders know?
- What code patterns should be standard?
- What test requirements are essential?
- What similar stories exist?
</objective>

<success_criteria>
- [ ] Analyzed review findings
- [ ] Identified preventable issues
- [ ] Determined which playbook(s) to update
- [ ] Return structured proposal
</success_criteria>

<completion_format>
{
  "agent": "reflection",
  "story_key": "{{story_key}}",
  "learnings": [
    {
      "issue": "SQL injection in query builder",
      "root_cause": "Builder used string concatenation (didn't know pattern)",
      "prevention": "Playbook should document: always use parameterized queries",
      "applies_to": "database queries, API endpoints with user input"
    },
    {
      "issue": "Missing edge case tests for empty arrays",
      "root_cause": "Test Quality Agent found gap",
      "prevention": "Playbook should require: test null/empty/invalid for all inputs",
      "applies_to": "all data processing functions"
    }
  ],
  "playbook_proposal": {
    "action": "update_existing" | "create_new",
    "playbook": "docs/playbooks/implementation-playbooks/database-api-patterns.md",
    "module": "api/database",
    "updates": {
      "common_gotchas": [
        "Never concatenate user input into SQL - use parameterized queries",
        "Test edge cases: null, undefined, [], '', invalid input"
      ],
      "code_patterns": [
        "db.query(sql, [param1, param2]) âœ“",
        "sql + userInput âœ—"
      ],
      "test_requirements": [
        "Test SQL injection attempts: expect(query(\"' OR 1=1--\")).toThrow()",
        "Test empty inputs: expect(fn([])).toHandle() or .toThrow()"
      ],
      "related_stories": ["{{story_key}}"]
    }
  }
}

Save to: docs/sprint-artifacts/completions/{{story_key}}-reflection.json
</completion_format>
\`
})
\`\`\`

**Wait for completion.**

**Review playbook proposal:**
\`\`\`bash
REFLECTION="docs/sprint-artifacts/completions/{{story_key}}-reflection.json"
ACTION=$(jq -r '.playbook_proposal.action' "$REFLECTION")
PLAYBOOK=$(jq -r '.playbook_proposal.playbook' "$REFLECTION")

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ“ Playbook Update Proposal"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Action: $ACTION"
echo "Playbook: $PLAYBOOK"
echo ""
jq -r '.learnings[] | "- \(.issue)\n  Prevention: \(.prevention)"' "$REFLECTION"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
\`\`\`

If \`auto_apply_updates: true\` in config:
- Read playbook (or create from template if new)
- Use Edit tool to add learnings to sections
- Commit playbook update

If \`auto_apply_updates: false\` (default):
- Display proposal for manual review
- User can apply later with \`/update-playbooks {{story_key}}\`
</step>

</process>

<failure_handling>
**Builder fails (Phase 1):** Don't spawn verification. Report failure and halt.
**Inspector fails (Phase 2):** Still collect other reviewer findings.
**Test Quality fails:** Add issues to Builder fix list.
**Coverage below threshold:** Add to Builder fix list.
**Reviewers find CRITICAL:** Builder MUST fix when resumed.
**Inspector fails (Phase 4):** Resume Builder again (iterative loop, max 3 iterations).
**Builder resume fails:** Report unfixed issues. Manual intervention.
**Reconciliation fails:** Fix with Edit tool, re-verify.
</failure_handling>

<complexity_routing>
| Complexity | Phase 2 Agents | Total | Security |
|------------|----------------|-------|----------|
| micro | Inspector + Test Quality + 2 Reviewers | 4 agents | Security Reviewer + Architect |
| standard | Inspector + Test Quality + 3 Reviewers | 5 agents | Security + Logic + Architect |
| complex | Inspector + Test Quality + 4 Reviewers | 6 agents | Security + Logic + Architect + Quality |

**All verification agents spawn in parallel (single message)**
</complexity_routing>

<success_criteria>
- [ ] Phase 0: Playbooks loaded (if available)
- [ ] Phase 1: Builder spawned, agent_id saved
- [ ] Phase 2: All verification agents completed in parallel
- [ ] Phase 2.5: Coverage gate passed
- [ ] Phase 3: Builder resumed with consolidated findings
- [ ] Phase 4: Inspector recheck passed
- [ ] Phase 5: Orchestrator reconciled with Inspector evidence
- [ ] Phase 6: Playbook reflection completed
- [ ] Implementation commit exists (from Phase 3)
- [ ] Reconciliation commit exists (from Phase 5)
- [ ] Story tasks checked with code citations
- [ ] Dev Agent Record filled
- [ ] Coverage â‰¥ {{coverage_threshold}}%
- [ ] Sprint status: done
</success_criteria>

<improvements_v4>
1. âœ… Resume Builder for fixes (v3.2+) - 50-70% token savings
2. âœ… Inspector provides code citations (v4.0) - file:line evidence for every task
3. âœ… Removed "hospital-grade" framing (v4.0) - kept disciplined gates
4. âœ… Micro stories get 2 reviewers + security scan (v3.2+) - not zero
5. âœ… Test Quality Agent (v4.0) + Coverage Gate (v4.0) - validates test quality and enforces threshold
6. âœ… Playbook query (v4.0) before Builder + reflection (v4.0) after - continuous learning
7. âœ… Reconciliation commit (v4.2) - commits story file updates after validation
</improvements_v4>
